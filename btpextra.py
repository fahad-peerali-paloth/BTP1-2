# -*- coding: utf-8 -*-
"""BTPextra.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cluCLRTSumOGSQEN-5nS6BNuTWae7oXe
"""

import pandas as pd
from sklearn.utils import shuffle

"""# New Section"""

df = pd.read_csv('/content/data_btp_x.csv')

df.to_csv("data.csv")

df.rename(columns = {'Lambda/L':'lamdabyl'}, inplace = True)

df.rename(columns = {'H/Lamda':'hbylamda'}, inplace = True)

df = shuffle(df, random_state=42)

df.head()

from sklearn.datasets import load_diabetes
from sklearn.linear_model import RidgeCV

#df = pd.DataFrame({'lamdabyl': df["lamdabyl"],
#                   'Fn': df["F_n"],
#                   'hbylamda':df["hbylamda"],
#                   'beta':df["Beta"],
#                   'max pressure':df["P_max"]
#                   })

df

#MAX_ytrain=df.iloc[:, 5].values
#shu=len(MAX_ytrain) // 13

#for i in range(shu):
#    start_idx = i * 13
#    end_idx = (i + 1) * 13
#    y_trans = MAX_ytrain[start_idx:end_idx]
#    y_trans_2d = y_trans.reshape(-1, 1)
#    ytrain_scaled=scaler_Y.fit_transform(y_trans_2d)
#    ytrain_scaled_1d = ytrain_scaled.flatten()
#    MAX_ytrain[start_idx:end_idx]=ytrain_scaled_1d

df

max_pressure= df["P_max"]/(1025*9.81*0.3134)

max_pressure

"""**ATTRIBUTE ANALYSIS**"""

from scipy import stats
a=stats.ttest_ind(df['beta'],df['max pressure'])[1]
b=stats.ttest_ind(df['hbylamda'],df['max pressure'])[1]
c=stats.ttest_ind(df['Fn'],df['max pressure'])[1]
d=stats.ttest_ind(df['lamdabyl'],df['max pressure'])[1]

dict_a = {
   'beta_max pressure,':[a],
   'hbylamda_max pressure': [b],
   'Fn_max pressure':[c],
   'lamdabyl_max pressure':[d]
}

df_pvalue = pd.DataFrame.from_dict(dict_a, orient ='index')
df_pvalue.columns = ['P value']
df_pvalue

e=df['max pressure'].corr(df['beta'])
f=df['max pressure'].corr(df['hbylamda'])
g=df['max pressure'].corr(df['Fn'])
h=df['max pressure'].corr(df['lamdabyl'])

dict_b = {
   'beta_max pressure,':[e],
   'hbylamda_max pressure': [f],
   'Fn_max pressure':[g],
   'lamdabyl_max pressure':[h]
}

df_corre = pd.DataFrame.from_dict(dict_b, orient ='index')
# df.corr(method ='pearson')
df_corre.columns = ['Pearson Correlation']
df_corre

max_pressure

"""**TRAIN TEST SPLIT**"""

data_x = df.iloc[:, 1:5]

data_x =data_x.to_numpy()

data_y=df.iloc[:, 5]/(1025*9.81*0.3134)

data_y =data_y.to_numpy()

data_y

data_train=data_x
data_y_train=data_y

data_train = data_x[:-20]
data_test = data_x[-20:]

data_y_train =data_y[:-20]
data_y_test = data_y[-20:]

"""**VISUALIZATION**"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(x='max pressure',
                y='lamdabyl', data=df)
plt.savefig('scatter_plot1.png')

sns.scatterplot(x='max pressure',
                y='hbylamda', data=df)
plt.savefig('scatter_plot2.png')

sns.scatterplot(x='max pressure',
                y='beta', data=df)
plt.savefig('scatter_plot3.png')

sns.scatterplot(x='max pressure',
                y='Fn', data=df)
plt.savefig('scatter_plot4.png')

"""**odinary least squares for all 4 attributes**"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

regr = linear_model.LinearRegression()

regr.fit(data_train, data_y_train)

data_y_pred = regr.predict(data_test)

print("Coefficients: \n", regr.coef_)
# The mean squared error
print("Mean squared error: %f" % mean_squared_error(data_y_test, data_y_pred))
score_plr=mean_squared_error(data_y_test, data_y_pred)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" % r2_score(data_y_test, data_y_pred))

RMSE_linear = mean_squared_error(data_y_test, data_y_pred)
r2_ordinary = r2_score(data_y_test, data_y_pred)

"""lasso"""

from sklearn.preprocessing import scale
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
from sklearn.linear_model import MultiTaskLassoCV

lasso_reg = LassoCV().fit(data_train, data_y_train)

data_y_predlasso=lasso_reg.predict(data_test)

print("Coefficients: \n", lasso_reg.coef_)
# The mean squared error
print("Mean squared error: %f" % mean_squared_error(data_y_test, data_y_predlasso))
score_plrl=mean_squared_error(data_y_test, data_y_predlasso)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" % r2_score(data_y_test, data_y_predlasso))

RMSE_lasso = mean_squared_error(data_y_test, data_y_predlasso)
r2_lasso = r2_score(data_y_test, data_y_predlasso)

"""ridge"""

ridge_reg = RidgeCV().fit(data_train, data_y_train)

data_y_predridge=ridge_reg.predict(data_test)

print("Coefficients: \n", ridge_reg.coef_)
# The mean squared error
print("Mean squared error: %f" % mean_squared_error(data_y_test, data_y_predridge))
score_plrr=mean_squared_error(data_y_test, data_y_predridge)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" % r2_score(data_y_test, data_y_predridge))

RMSE_ridge= mean_squared_error(data_y_test, data_y_predridge)
r2_ridge = r2_score(data_y_test, data_y_predridge)

"""BayesianRidge

"""

clf = linear_model.BayesianRidge()

clf.fit(data_train, data_y_train)

data_y_bay=clf.predict(data_test)

print("Coefficients: \n", clf.coef_)
# The mean squared error
print("Mean squared error: %f" % mean_squared_error(data_y_test, data_y_bay))
score_plrr=mean_squared_error(data_y_test, data_y_bay)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" % r2_score(data_y_test, data_y_bay))

RMSE_Bayesian= mean_squared_error(data_y_test, data_y_bay)
r2_Bayesian = r2_score(data_y_test, data_y_bay)

"""**PCA**"""

from sklearn.preprocessing import scale
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA

X_train_scaled = scale(data_train)
X_test_scaled = scale(data_test)

cv = KFold(n_splits=10, shuffle=True, random_state=42)

# Linear Regression
lin_reg = LinearRegression().fit(X_train_scaled, data_y_train)
lr_score_train = -1 * cross_val_score(lin_reg, X_train_scaled, data_y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
lr_score_test = mean_squared_error(data_y_test, lin_reg.predict(X_test_scaled))

# Lasso Regression
lasso_reg = LassoCV().fit(X_train_scaled, data_y_train)
lasso_score_train = -1 * cross_val_score(lasso_reg, X_train_scaled, data_y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
lasso_score_test = mean_squared_error(data_y_test, lasso_reg.predict(X_test_scaled))

# Ridge Regression
ridge_reg = RidgeCV().fit(X_train_scaled, data_y_train)
ridge_score_train = -1 * cross_val_score(ridge_reg, X_train_scaled, data_y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
ridge_score_test = mean_squared_error(data_y_test, ridge_reg.predict(X_test_scaled))

print("Mean squared error: %f" % lr_score_test)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" %  r2_score(data_y_test, lin_reg.predict(X_test_scaled)))
r2_crosslinear = r2_score(data_y_test, lin_reg.predict(X_test_scaled))

print("Mean squared error: %f" % lasso_score_test)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" %  r2_score(data_y_test, lasso_reg.predict(X_test_scaled)))
r2_crosslasso = r2_score(data_y_test, lasso_reg.predict(X_test_scaled))

print("Mean squared error: %f" % ridge_score_test)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" %  r2_score(data_y_test, ridge_reg.predict(X_test_scaled)))
r2_crossridge =  r2_score(data_y_test, ridge_reg.predict(X_test_scaled))

lasso_score_test

ridge_score_test

# Generate all the principal components
pca = PCA() # Default n_components = min(n_samples, n_features)
X_train_pc = pca.fit_transform(X_train_scaled)

# View first 5 rows of all principal components
pd.DataFrame(pca.components_.T).to_csv("pca.csv")

pca.explained_variance_ratio_

import matplotlib.pyplot as plt

lin_reg = LinearRegression()

# Create empty list to store RMSE for each iteration
rmse_list = []

# Loop through different count of principal components for linear regression
for i in range(1, X_train_pc.shape[1]+1):
    rmse_score = -1 * cross_val_score(lin_reg,
                                      X_train_pc[:,:i], # Use first k principal components
                                      data_y_train,
                                      cv=cv,
                                      scoring='neg_root_mean_squared_error').mean()
    rmse_list.append(rmse_score)

# Visual analysis - plot RMSE vs count of principal components used
plt.plot(rmse_list, '-o')
plt.xlabel('Number of principal components in regression')
plt.ylabel('RMSE')
plt.title('Quality')
plt.xlim(xmin=-1);
plt.xticks(np.arange(X_train_pc.shape[1]), np.arange(1, X_train_pc.shape[1]+1))
plt.axhline(y=lr_score_train, color='g', linestyle='-');
plt.savefig('plot5.png')

# Visually determine optimal number of principal components
best_pc_num = 3

# Train model with first 9 principal components
lin_reg_pc = LinearRegression().fit(X_train_pc[:,:best_pc_num], data_y_train)

# Get cross-validation RMSE (train set)
pcr_score_train = -1 * cross_val_score(lin_reg_pc,
                                       X_train_pc[:,:best_pc_num],
                                       data_y_train,
                                       cv=cv,
                                       scoring='neg_root_mean_squared_error').mean()

# Train model on training set
lin_reg_pc = LinearRegression().fit(X_train_pc[:,:best_pc_num], data_y_train)

# Get first 9 principal components of test set
X_test_pc = pca.transform(X_test_scaled)[:,:best_pc_num]

# Predict on test data
preds = lin_reg_pc.predict(X_test_pc)
pcr_score_test = mean_squared_error(data_y_test, preds)

print("Mean squared error: %f" % pcr_score_test)
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %f" %  r2_score(data_y_test, preds))
r2_pcr = r2_score(data_y_test, preds)

dict_c = {
   'Linear Regression':[score_plr,r2_ordinary],
   'Lasso Regression':[score_plrl,r2_lasso],
   'Ridge Regression':[score_plrr,r2_ridge],
   'Cross_Validation Regression': [lr_score_test,r2_crosslinear],
   'Cross_Validation Lasso_Regression':[lasso_score_test,r2_crosslasso],
   'Cross_Validation Ridge_Regression':[ridge_score_test,r2_crossridge],
   'Principal Component Regression':[pcr_score_test,r2_pcr],
   'DecisionTreeRegressor_Depth2':[decision_score_test,r2_depth2],
   'DecisionTreeRegressor_Depth5':[decision_score_test_2,r2_depth3],
   'RandomForestRegressor':[rn_score_test,r2_rn],
   'RandomForestRegressor_Hyperparameter Tuning':[rn_score_test_hy,r2_hy]

}

df_com = pd.DataFrame.from_dict(dict_c, orient ='index')

df_com.columns =['RMSE','R2']

data_train.shape

import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

# Create a random dataset

X = data_train
y = data_y_train


# Fit regression model
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=3)
regr_1.fit(X, y)
regr_2.fit(X, y)

# Predict
X_test = data_test
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
decision_score_test = mean_squared_error(data_y_test, y_1)
decision_score_test_2 = mean_squared_error(data_y_test, y_2)
r2_depth2= r2_score(data_y_test, y_1)
r2_depth3 = r2_score(data_y_test, y_2)
decision_score_test,decision_score_test_2,r2_score(data_y_test, y_1),r2_score(data_y_test, y_2)

df_com

df_com.to_csv("result.csv")

X.shape

y.shape

data_train[:,:1].shape

data_y_train.shape

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
X, y = make_regression(n_features=4, n_informative=2,
                       random_state=1, shuffle=False)
regr = RandomForestRegressor(max_depth=3, random_state=1)
X = data_train
y = data_y_train
regr.fit(X, y)

y1=regr.predict(X_test)

rn_score_test = mean_squared_error(data_y_test, y_1)
r2_rn= r2_score(data_y_test, y1)
rn_score_test,r2_score(data_y_test, y1)

!pip install scikit-optimize

from sklearn.ensemble import RandomForestRegressor
from skopt import BayesSearchCV
from skopt.space import Real, Integer

param_dist = {
    'n_estimators': Integer(1, 500),
    'max_depth': Integer(1, 50),
    'min_samples_split': Integer(2, 10),
    'min_samples_leaf': Integer(1, 5),
    'max_features': Real(0.1, 1.0),
    'bootstrap': [True, False]
}

rf = RandomForestRegressor()

bayes_search = BayesSearchCV(rf, param_dist, n_iter=10, cv=5, n_jobs=-1, random_state=42)

X = data_train
y = data_y_train

bayes_search.fit(X, y)

print("Best hyperparameters: ", bayes_search.best_params_)

rf_best = RandomForestRegressor(**bayes_search.best_params_)

# Fit the new Random Forest Regressor object on the training data
rf_best.fit(X, y)

# Evaluate the performance of the new Random Forest Regressor object on the testing data
y_pred = rf_best.predict(X_test)

rn_score_test_hy = mean_squared_error(data_y_test, y_pred)
r2_hy = r2_score(data_y_test, y_pred)
rn_score_test_hy,r2_score(data_y_test, y_pred)

from joblib import dump

# Save the model to a file
dump(rf_best, 'random_forest_model.joblib')

from google.colab import files

# Download the model file
files.download('random_forest_model.joblib')

data_test

data_y_test

y_pred

import matplotlib.pyplot as plt

# Plot the predicted values against the actual values of the testing data
plt.scatter(data_y_test, y_pred)
plt.xlabel('CFD Values')
plt.ylabel('Predicted Values')
plt.title('Random Forest Regressor')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Sort the actual values and predicted values by ascending order of the actual values
idx = np.argsort(data_y_test)
y_test_sorted = data_y_test[idx]
y_pred_sorted = y_pred[idx]

# Plot the actual values and predicted values as lines
plt.plot(y_test_sorted, label='Actual Values')
plt.plot(y_pred_sorted, label='Predicted Values')
plt.xlabel('Observations')
plt.ylabel('Target Values')
plt.title('Random Forest Regressor')
plt.legend()
plt.show()

df_comp = pd.read_csv('/content/testing_data.csv')

df_comp= df_comp.to_numpy()

df_comp

y_pred_fromcf = rf_best.predict(df_comp)

y_pred_fromcfd *= (1025*9.81*0.3354)

y_pred_fromcfd

df_cfd = pd.DataFrame(y_pred_fromcfd)

df_cfd